{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fasttext.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmBilFF7X47abCQe8d//iE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6-rWPYz9PJ3X"},"source":["#fasttext installation\n","!pip install fasttext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNFPC0dUOqnb"},"source":["# Importing libraries\n","# splitting in train-validation-test sets in a stratified manner.\n","from sklearn.model_selection import train_test_split\n","import numpy as np, pandas as pd\n","# NLP Preprocessing\n","from gensim.utils import simple_preprocess\n","import fasttext\n","import csv\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"TUewsNA9Kr2x","executionInfo":{"status":"ok","timestamp":1630775402303,"user_tz":-180,"elapsed":299,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"7ccffaed-45d6-4550-8c07-86bc140b3269"},"source":["import json\n","import pandas as pd\n","\n","label2id = {\n","    'NONE': 0,\n","    'EVIDENCE': 1,\n","    'CLAIM': 2}\n","\n","def load_corpus(path, label_mapping=None):\n","    with open(path) as fp:\n","        corpus = json.load(fp)\n","\n","    documents, texts, labels = [], [], []\n","    for abstract in corpus:\n","        documents.append(abstract)\n","        texts.append(corpus[abstract]['sentences'])\n","        if isinstance(label_mapping, dict):\n","            labels.append(\n","                [label_mapping[str(l).upper()]\n","                    for l in corpus[abstract]['labels']])\n","        else:\n","            labels.append([str(l).upper() for l in corpus[abstract]['labels']])\n","\n","    assert len(texts) == len(labels)\n","    data = pd.DataFrame(\n","        zip(documents, texts, labels),\n","        columns=['document', 'sentences', 'labels'])\n","\n","    return data\n","\n","data_v1 = load_corpus('dataset.json') #, label_mapping=label2id)\n","data_v3= load_corpus('dataset_aueb_argument_v3.json') #, label_mapping=label2id)\n","print(f'Dataset length: {len(data_v1)+ len(data_v3)} abstracts')\n","data_v1.head(5)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset length: 2686 abstracts\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>sentences</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>DEI_G2B1_15.txt</td>\n","      <td>[Gender Differences in Anxiety and Depression ...</td>\n","      <td>[NONE, NONE, NONE, NONE, NONE, NONE, NONE, NON...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>DEI_G2B1_23.txt</td>\n","      <td>[Women's economic empowerment, participation i...</td>\n","      <td>[NONE, NONE, NONE, NONE, NONE, NONE, NONE, EVI...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>DEI_G2B1_24.txt</td>\n","      <td>[Forced sterilization of women as discriminati...</td>\n","      <td>[NONE, NONE, NONE, NONE, NONE, NONE, NONE, NON...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>DEI_G2B1_31.txt</td>\n","      <td>[Relationship of gender differences in prefere...</td>\n","      <td>[NONE, NONE, NONE, NONE, NONE, EVIDENCE, CLAIM]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>DEI_G2B1_39.txt</td>\n","      <td>[Womenâ€™s Assessments of Gender Equality, Abstr...</td>\n","      <td>[NONE, NONE, NONE, NONE, EVIDENCE, EVIDENCE, C...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          document  ...                                             labels\n","0  DEI_G2B1_15.txt  ...  [NONE, NONE, NONE, NONE, NONE, NONE, NONE, NON...\n","1  DEI_G2B1_23.txt  ...  [NONE, NONE, NONE, NONE, NONE, NONE, NONE, EVI...\n","2  DEI_G2B1_24.txt  ...  [NONE, NONE, NONE, NONE, NONE, NONE, NONE, NON...\n","3  DEI_G2B1_31.txt  ...    [NONE, NONE, NONE, NONE, NONE, EVIDENCE, CLAIM]\n","4  DEI_G2B1_39.txt  ...  [NONE, NONE, NONE, NONE, EVIDENCE, EVIDENCE, C...\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"YFzVJQRmQQXR"},"source":["## Split Documents\n","For the cases we want the sentences separated, the following splits the documents. I keep the same document index in a new column in order to re-group the sentences to a document (e.g., after predictions)."]},{"cell_type":"code","metadata":{"id":"i4tdC6HeQVJ-"},"source":["#@title Split to sentences\n","sentences_v1 = data_v1['sentences'].explode().reset_index().rename(\n","    columns={'index': 'doc_id', 'sentences': 'sentence'})\n","\n","sentences_v3 = data_v3['sentences'].explode().reset_index().rename(\n","    columns={'index': 'doc_id', 'sentences': 'sentence'})\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_lqJUDgmZ3H"},"source":["#@title and the corresponding labels\n","labels_v1 = data_v1['labels'].explode().reset_index().rename(\n","    columns={'index': 'doc_id', 'labels': 'label'})\n","\n","labels_v3 = data_v3['labels'].explode().reset_index().rename(\n","    columns={'index': 'doc_id', 'labels': 'label'})\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"hMwtxEIpQZmr","executionInfo":{"status":"ok","timestamp":1630597814543,"user_tz":-180,"elapsed":19,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"bdd77471-5554-45d3-84d6-e1eac9df20ba"},"source":["\n","#conver the sentence column from object to string\n","sentences_v1['sentence'] = sentences_v1['sentence'].astype(\"string\")\n","sentences_v3['sentence'] = sentences_v3['sentence'].astype(\"string\")\n","# remove any blanks from the start and the and of the string\n","sentences_v1['sentence'] = sentences_v1['sentence'].str.strip()\n","sentences_v3['sentence'] = sentences_v3['sentence'].str.strip()\n","sentences_v1[\"label\"]=labels_v1[\"label\"]\n","sentences_v3[\"label\"]=labels_v3[\"label\"]\n","#Coancatenate the two dataframes\n","data = sentences_v1.append(sentences_v3, ignore_index=True)\n","#repalce the none labels with neither\n","data['label'] = data['label'].str.replace('NONE','NEITHER')\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>doc_id</th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Gender Differences in Anxiety and Depression b...</td>\n","      <td>NEITHER</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>Abstract</td>\n","      <td>NEITHER</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>Background/aims: The aim of this prospective s...</td>\n","      <td>NEITHER</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>Methods: AUD severity, state and trait anxiety...</td>\n","      <td>NEITHER</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Follow-up assessments were performed at 6 and ...</td>\n","      <td>NEITHER</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   doc_id                                           sentence    label\n","0       0  Gender Differences in Anxiety and Depression b...  NEITHER\n","1       0                                           Abstract  NEITHER\n","2       0  Background/aims: The aim of this prospective s...  NEITHER\n","3       0  Methods: AUD severity, state and trait anxiety...  NEITHER\n","4       0  Follow-up assessments were performed at 6 and ...  NEITHER"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"EBpYRKdQQZsQ"},"source":["#create new train and test set for fasttext\n","X_train_val, X_test, y_train_val, y_test = train_test_split(data['sentence'],\n","                                                            data['label'],\n","                                                            test_size=0.15,\n","                                                            random_state=42,\n","                                                            stratify=data['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHURqrGqQZvJ"},"source":["#create new train and validation set\n","X_train,X_val, y_train, y_val = train_test_split(X_train_val,\n","                                                y_train_val,\n","                                                test_size=0.15,\n","                                                random_state=42,\n","                                                stratify=y_train_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZg4mkAJQZyC"},"source":["\n","# Importing the dataset\n","df_train=pd.concat([X_train,y_train], axis=1)\n","df_val =pd.concat([X_val,y_val], axis=1)\n","df_test =pd.concat([X_test,y_test], axis=1)\n","\n","# NLP Preprocess\n","df_train.iloc[:, 0] = df_train.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\n","df_val.iloc[:, 0] = df_val.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\n","df_test.iloc[:, 0] = df_test.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\n","\n","# #import nlp package for stopwords\n","# import nltk\n","# from nltk.corpus import stopwords\n","# nltk.download('stopwords')\n","# stop = stopwords.words('english')\n","# #Remove stopwords\n","# df_train['sentence'] = df_train['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","# df_val['sentence'] = df_val['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","# df_test['sentence'] = df_test['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","\n","# Prefixing each row of the category column with '__label__'\n","df_train.iloc[:, 1] = df_train.iloc[:, 1].apply(lambda x: '__label__' + x)\n","df_val.iloc[:, 1] = df_val.iloc[:, 1].apply(lambda x: '__label__' + x)\n","df_test.iloc[:, 1] = df_test.iloc[:, 1].apply(lambda x: '__label__' + x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hVoDVO-by0no"},"source":[""]},{"cell_type":"code","metadata":{"id":"04wS1OShQaUa"},"source":["# Saving the CSV file as a text file to train/valid the classifier\n","\n","df_train[['label', 'sentence']].to_csv('train.txt', index = False, sep = ' ', header = None, \n","                                          quoting = csv.QUOTE_NONE, quotechar = \"\",escapechar = \" \")\n","\n","df_test[['label', 'sentence']].to_csv('test.txt', index = False, sep = ' ', header = None, \n","                                     quoting = csv.QUOTE_NONE, quotechar = \"\", escapechar = \" \")\n","\n","df_val[['label', 'sentence']].to_csv('valid.txt', index = False, sep = ' ', header = None, \n","                                     quoting = csv.QUOTE_NONE, quotechar = \"\", escapechar = \" \")\n","\n","\n","# Training the fastText classifier\n","model = fasttext.train_supervised('train.txt',autotuneValidationFile='valid.txt', autotuneDuration=600)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtgykxRNEXz9","executionInfo":{"status":"ok","timestamp":1630599436158,"user_tz":-180,"elapsed":2919,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"5961aa3f-a691-4f7a-9def-3daa00c80efc"},"source":["model.test('train.txt')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(23122, 0.8169708502724677, 0.8169708502724677)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jt5NrmIj4Ok0","executionInfo":{"status":"ok","timestamp":1630599440277,"user_tz":-180,"elapsed":1010,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"a5edf109-a972-4dd7-cfcb-0fc771bb243a"},"source":["model.test('valid.txt')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4081, 0.7755452095074736, 0.7755452095074736)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDWzPMlCQaX1","executionInfo":{"status":"ok","timestamp":1630599448445,"user_tz":-180,"elapsed":253,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"efa96e6b-923a-4e1c-951c-a264d8a281cd"},"source":["#find hyperprameters for optimized model\n","args_obj = model.f.getArgs()\n","for hparam in dir(args_obj):\n","    if not hparam.startswith('__'):\n","        print(f\"{hparam} -> {getattr(args_obj, hparam)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["autotuneDuration -> 600\n","autotuneMetric -> f1\n","autotuneModelSize -> \n","autotunePredictions -> 1\n","autotuneValidationFile -> valid.txt\n","bucket -> 3179202\n","cutoff -> 0\n","dim -> 109\n","dsub -> 2\n","epoch -> 12\n","input -> train.txt\n","label -> __label__\n","loss -> loss_name.softmax\n","lr -> 0.1147434290700611\n","lrUpdateRate -> 100\n","maxn -> 6\n","minCount -> 1\n","minCountLabel -> 0\n","minn -> 3\n","model -> model_name.supervised\n","neg -> 5\n","output -> \n","pretrainedVectors -> \n","qnorm -> False\n","qout -> False\n","retrain -> False\n","saveOutput -> False\n","seed -> 0\n","setManual -> <bound method PyCapsule.setManual of <fasttext_pybind.args object at 0x7fde46dab270>>\n","t -> 0.0001\n","thread -> 1\n","verbose -> 2\n","wordNgrams -> 1\n","ws -> 5\n"]}]},{"cell_type":"code","metadata":{"id":"iie8wtdKRQk_"},"source":["#create a list with sentence and labels for train and test set\n","with open(\"train.txt\") as file_in:\n","    lines_tr = []\n","    for line in file_in:\n","        line = line.rstrip('\\n')\n","        lines_tr.append(line)\n","\n","with open(\"test.txt\") as file_in:\n","    lines_te = []\n","    for line in file_in:\n","        line = line.rstrip('\\n')\n","        lines_te.append(line)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9iTyvXRERQni"},"source":["#predictions for training set\n","train_pred=[]\n","for i in lines_tr:\n","  x=model.predict(i)\n","  train_pred.append(x)# append tuple to list\n","\n","tr_pred=[]\n","for a,b in train_pred:#tuple unpacking\n","   a=''.join(a)\n","   tr_pred.append(a)\n","\n","#predictions for test set\n","test_pred=[]\n","for i in lines_te:\n","  x=model.predict(i)\n","  test_pred.append(x)#append tuple to list\n","\n","ts_pred=[]\n","for a,b in test_pred:#tuple unpacking\n","   a=''.join(a)\n","   ts_pred.append(a)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uPozHpzRQp8","executionInfo":{"status":"ok","timestamp":1630599467972,"user_tz":-180,"elapsed":1063,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"f73f9d33-427f-42b6-8192-af15d916e5e0"},"source":["\n","print(metrics.classification_report(tr_pred,df_train['label']))\n","print()\n","print(metrics.confusion_matrix(tr_pred,df_train['label']))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                   precision    recall  f1-score   support\n","\n","   __label__CLAIM       0.34      0.71      0.46      1193\n","__label__EVIDENCE       0.61      0.76      0.67      3590\n"," __label__NEITHER       0.95      0.84      0.89     18339\n","\n","         accuracy                           0.82     23122\n","        macro avg       0.63      0.77      0.67     23122\n","     weighted avg       0.86      0.82      0.83     23122\n","\n","\n","[[  846   108   239]\n"," [  265  2721   604]\n"," [ 1359  1657 15323]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msp5eeQMRQtc","executionInfo":{"status":"ok","timestamp":1630599482299,"user_tz":-180,"elapsed":247,"user":{"displayName":"Georgios Mavridis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuNeXMs3Lbw-bAMQ8PktpurU3LNPZlESNInMAK0w=s64","userId":"07496280154128810280"}},"outputId":"a5ef3b6a-a341-4797-b039-4ed1a1bcf703"},"source":["\n","print(metrics.classification_report(ts_pred,df_test['label']))\n","print()\n","print(metrics.confusion_matrix(ts_pred,df_test['label']))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                   precision    recall  f1-score   support\n","\n","   __label__CLAIM       0.27      0.54      0.36       253\n","__label__EVIDENCE       0.51      0.65      0.57       737\n"," __label__NEITHER       0.91      0.80      0.85      3811\n","\n","         accuracy                           0.77      4801\n","        macro avg       0.56      0.67      0.60      4801\n","     weighted avg       0.82      0.77      0.79      4801\n","\n","\n","[[ 137   32   84]\n"," [  49  479  209]\n"," [ 327  421 3063]]\n"]}]}]}